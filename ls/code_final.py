# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NKc7vTrw9_qP_uZcji6-EGKuhXmkX3zc
"""

# ✅ Disable Weights & Biases logging
import os
os.environ["WANDB_DISABLED"] = "true"

# ✅ Skip reinstalling packages — assume already installed
# !pip install evaluate transformers
# !pip install --upgrade datasets fsspec
!pip install fsspec==2023.6.0

import transformers
print(transformers.__version__)  # Should be 4.53.1 or later
# !pip install --upgrade datasets fsspec
from datasets import load_dataset
from transformers import TrainingArguments, AutoTokenizer, GPT2LMHeadModel, Trainer
import math

# # Clear cache
!rm -rf ~/.cache/huggingface/datasets/wikitext
# ✅ Load dataset (no cache clear to avoid re-download)
dataset = load_dataset("wikitext", name="wikitext-2-v1")

# ✅ Tokenization
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no pad token

def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

tokenized = dataset.map(tokenize, batched=True, remove_columns=["text"])
block_size = 128

def group_texts(examples):
    concatenated = sum(examples['input_ids'], [])
    total_len = (len(concatenated) // block_size) * block_size
    result = {
        "input_ids": [concatenated[i:i + block_size] for i in range(0, total_len, block_size)],
        "labels": [concatenated[i:i + block_size] for i in range(0, total_len, block_size)]
    }
    return result

lm_dataset = tokenized.map(group_texts, batched=True)

# ✅ Load model (only once)
model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))

# ✅ Training config (optimized for speed)

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=1,
    logging_strategy="no",             # ✅ disables logging
    save_total_limit=1,
    fp16=True,
    eval_strategy="epoch",
    push_to_hub=False
)

# ✅ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["validation"]
)

# ✅ Train
trainer.train()

from google.colab import drive
drive.mount('/content/drive')

# Save to Google Drive
model.save_pretrained("/content/drive/MyDrive/gpt2-finetuned")
tokenizer.save_pretrained("/content/drive/MyDrive/gpt2-finetuned")

# ✅ Evaluate
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

from transformers import pipeline

text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

prompt = "Worlds biggest fear is blood because"
output = text_generator(prompt, max_length=10, num_return_sequences=1)
print("Generated Text:", output[0]["generated_text"])

import torch
from torch.nn.functional import cross_entropy
from tqdm import tqdm

model.eval()  # Set model to evaluation mode

k = 5  # For Top-5 accuracy
topk_correct = 0
total = 0

val_dataset = lm_dataset["validation"]
subset = val_dataset.select(range(100))  # Use small subset to speed up

for example in tqdm(subset):
    input_ids = torch.tensor([example["input_ids"]]).to(model.device)
    labels = torch.tensor([example["labels"]]).to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits  # [1, seq_len, vocab_size]

    # Skip first token, compare prediction at t with true at t+1
    for t in range(logits.shape[1] - 1):
        preds = logits[0, t]
        true_token = labels[0, t + 1]

        topk = torch.topk(preds, k=k).indices  # Top-k token IDs
        if true_token in topk:
            topk_correct += 1
        total += 1

topk_accuracy = topk_correct / total
print(f"Top-{k} Accuracy: {topk_accuracy:.4f}")



# from transformers import GPT2LMHeadModel, AutoTokenizer

# model = GPT2LMHeadModel.from_pretrained("/content/drive/MyDrive/gpt2-finetuned")
# tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/gpt2-finetuned")